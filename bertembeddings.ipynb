{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huo_x\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    " \n",
    "# Set a random seed for PyTorch (for GPU as well)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(n, s):\n",
    "    pieces = s.split()\n",
    "    return (\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n))\n",
    "#with open('content-gpt3.5-turbo.txt', 'r') as f:\n",
    "#    with open(\"bertembedding-gpt3.5-turbo.txt\", 'w') as writer:\n",
    "#        for piece in splitter(5000, f.read()):\n",
    "#            writer.writelines(piece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_s = 'E:\\programming\\.vscode\\programming\\python\\openai\\paragraphs'\n",
    "path_d = 'E:\\programming\\.vscode\\programming\\python\\openai\\\\bertembedding-gemini'\n",
    "\n",
    "files= os.listdir(path_s)\n",
    "\n",
    "i=1\n",
    "for file in files:\n",
    "    with open(path_s + '\\\\' + file, 'r', encoding='gb2312', errors='ignore') as f:\n",
    "        with open(path_d + '\\\\' + str(i)+ \".txt\", 'w') as writer:\n",
    "            # Input text\n",
    "            text = [f.read()]\n",
    "            i+=1\n",
    "            # Tokenize and encode text using batch_encode_plus\n",
    "            # The function returns a dictionary containing the token IDs and attention masks\n",
    "            encoding = tokenizer.batch_encode_plus(\n",
    "                text,                    # List of input texts\n",
    "                padding=True,              # Pad to the maximum sequence length\n",
    "                truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "                return_tensors='pt',      # Return PyTorch tensors\n",
    "                add_special_tokens=True    # Add special tokens CLS and SEP\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids']  # Token IDs\n",
    "            # print input IDs\n",
    "            #print(f\"Input ID: {input_ids}\")\n",
    "            attention_mask = encoding['attention_mask']  # Attention mask\n",
    "            # print attention mask\n",
    "            #print(f\"Attention mask: {attention_mask}\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                sentence_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "            writer.writelines(str(sentence_embedding))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: geeksforgeeks is a technology website\n",
      "tokenized Text: ['geek', '##sf', '##org', '##ee', '##ks', 'is', 'a', 'technology', 'website']\n",
      "Sentence Embedding:\n",
      "tensor([[ 1.8937e-01,  1.7171e-01,  1.9121e-01,  1.7688e-01,  1.2271e-01,\n",
      "         -6.4131e-01, -8.4964e-02,  5.4482e-01, -7.0783e-02, -1.3358e-01,\n",
      "          1.9906e-02, -6.9319e-01,  1.1953e-01,  5.4765e-01, -3.5408e-01,\n",
      "          4.4257e-02, -4.4093e-01,  6.5740e-02, -2.7559e-01,  7.7596e-02,\n",
      "         -3.2382e-02, -2.2579e-01, -6.2654e-03,  5.7717e-01,  1.7169e-01,\n",
      "         -1.1326e-01,  8.1874e-02, -2.2233e-01, -1.1038e-01,  1.2131e-01,\n",
      "          4.0078e-01,  5.0243e-01, -5.9786e-02,  3.5135e-02, -4.7616e-01,\n",
      "         -2.7899e-01, -3.7338e-01, -1.0920e-02, -7.0375e-02,  8.3596e-01,\n",
      "         -2.8823e-01, -4.0729e-01,  1.5250e-01,  1.8642e-01, -1.5257e-01,\n",
      "         -2.3479e-01, -7.6374e-04,  7.0488e-02,  5.8966e-02, -7.6147e-02,\n",
      "         -6.2664e-01,  3.2932e-01, -4.0801e-01, -5.6394e-02,  3.9997e-01,\n",
      "          6.9548e-01,  2.2761e-01, -3.9538e-01, -2.4499e-01,  7.7599e-02,\n",
      "          4.1822e-02,  3.2957e-01,  2.3849e-02, -4.2077e-01, -8.5099e-02,\n",
      "         -2.6321e-03, -2.5511e-01,  6.4165e-01, -8.6938e-01, -4.7831e-02,\n",
      "         -1.5576e-01, -5.6606e-02, -3.0946e-01,  4.1495e-03,  1.2063e-01,\n",
      "         -5.5331e-02, -3.3316e-02,  4.6220e-01,  4.8201e-01, -1.3925e-01,\n",
      "         -3.9128e-01,  5.4548e-01,  2.2387e-01,  6.4880e-01,  1.3483e-01,\n",
      "          1.1051e-01, -1.5232e-01,  2.5350e-01, -7.4225e-02,  4.2467e-01,\n",
      "          1.4637e-01,  6.8883e-02,  8.1775e-01,  2.1548e-01,  4.3420e-01,\n",
      "          2.0008e-02,  1.5310e-01, -2.4319e-01, -2.2618e-01,  3.6738e-01,\n",
      "          2.6819e-01, -3.5792e-01, -2.7222e-01,  2.6436e-01, -4.9857e-01,\n",
      "          1.4652e-01,  1.0952e-01,  1.7481e-01,  2.3056e-01, -4.4057e-02,\n",
      "          5.8919e-01,  3.2801e-01,  1.1307e-01, -4.5706e-01, -3.1477e-02,\n",
      "         -1.0481e-01, -6.3432e-02, -6.5654e-02,  2.9326e-01, -3.8772e-01,\n",
      "          8.2719e-02,  5.3770e-01, -1.7936e-02,  9.7151e-01, -2.5701e-01,\n",
      "          2.6840e-01, -2.1922e-02,  2.3633e-01,  3.1392e-01,  1.9855e-01,\n",
      "          3.5851e-01,  3.6941e-01,  2.3721e-01, -2.0948e-01, -2.2454e-01,\n",
      "          3.5385e-01, -2.6809e-02, -2.7112e-02, -2.2862e-01,  1.5689e-01,\n",
      "         -2.6961e-01, -1.5077e-01, -4.1552e-01,  1.0427e-01,  7.8167e-02,\n",
      "          1.4415e-01, -5.6083e-02, -6.0168e-02, -3.0632e-03,  4.4369e-02,\n",
      "         -3.3349e-01, -9.5031e-02, -7.2579e-02, -5.8604e-01,  1.9253e-01,\n",
      "         -4.1588e-01,  3.5429e-01,  7.1490e-02,  5.9240e-01,  3.9180e-01,\n",
      "          9.1222e-02,  2.6041e-01, -3.2295e-01,  7.0163e-03, -1.9721e-01,\n",
      "         -1.4167e-02,  1.0163e-01,  3.2386e-01,  1.8259e-01,  3.3769e-01,\n",
      "         -4.9448e-02, -4.3383e-01,  6.7020e-01,  2.9250e-01,  1.9192e-01,\n",
      "          4.7968e-02,  3.7927e-01, -1.9805e-01,  3.2440e-01,  4.1841e-01,\n",
      "         -1.0713e+00,  8.4247e-02,  4.1163e-01, -8.5077e-02, -5.9665e-02,\n",
      "         -1.7434e-01,  1.6652e-01, -3.7229e-01, -1.9950e-01, -1.7523e-01,\n",
      "         -8.8977e-01, -1.1344e-01, -3.8684e-01,  1.2763e-01,  7.3095e-01,\n",
      "         -4.9454e-01, -2.4074e-01, -1.4424e-01, -2.7014e-01,  1.7086e-01,\n",
      "          3.4396e-01, -2.4531e-01, -2.1136e-01,  2.0240e-02, -3.9942e-01,\n",
      "          1.2952e-01,  1.1226e-01, -5.7060e-01, -4.8371e-01, -1.0390e-01,\n",
      "         -3.5547e-01,  4.6595e-01, -2.3297e-01,  2.9991e-01, -3.2495e-01,\n",
      "          1.5138e-01, -2.4728e-01,  8.3878e-02,  7.6153e-02, -1.5591e-02,\n",
      "          2.2388e-01, -1.6793e-01, -4.4050e-01,  3.2359e-01, -3.8423e-01,\n",
      "          5.5616e-01,  2.0126e-01, -2.4506e-01,  3.7929e-01,  3.5422e-02,\n",
      "         -2.1911e-01, -2.4205e-02,  5.7743e-01, -1.9896e-01,  2.8210e-01,\n",
      "         -3.2725e-01, -4.0777e-01, -2.6242e-01,  5.3971e-01, -2.6636e-01,\n",
      "         -3.0856e-01,  2.7123e-01,  3.1072e-01, -2.4607e-01,  3.7126e-01,\n",
      "         -6.5283e-02,  1.3788e-01,  3.0544e-01, -4.4831e-02, -1.7295e-01,\n",
      "          5.0672e-02, -5.1248e-01, -1.8063e-01, -1.5393e-01,  4.2214e-02,\n",
      "         -2.4592e-01,  1.6379e-01,  1.1994e-01, -2.9961e-01,  4.1352e-01,\n",
      "          5.4918e-01,  1.6643e-01,  7.4596e-01,  1.6841e-01, -2.8910e-01,\n",
      "         -3.7259e-01,  2.6167e-01, -4.2216e-03,  3.8304e-01, -9.3929e-02,\n",
      "          7.7625e-02, -8.1132e-02, -2.6619e-02,  1.4165e-01,  1.7700e-02,\n",
      "         -4.6295e-01,  1.6659e-01,  1.1678e-01, -8.7871e-02, -5.2805e-02,\n",
      "         -2.3536e-01,  3.7456e-01, -4.6282e-01, -2.1023e-01,  2.3684e-01,\n",
      "         -3.5250e-01, -8.4817e-03,  2.8585e-01, -3.4081e-01, -4.7486e-02,\n",
      "         -1.9331e-01,  3.2420e-01, -4.0202e-01, -2.0044e-01,  5.6418e-01,\n",
      "         -3.1831e-02,  2.3381e-02,  3.7145e-01,  1.2671e-01,  1.8203e-01,\n",
      "         -1.6075e-03, -1.8542e-01,  2.0331e-01,  3.5026e-01, -3.7477e-01,\n",
      "          1.2546e-01,  3.0113e-01, -7.7047e-01, -3.2481e+00, -2.1475e-01,\n",
      "          5.5433e-01, -3.5629e-01,  9.5424e-02,  5.8310e-02, -2.2663e-01,\n",
      "         -1.7844e-01, -3.4079e-01, -2.7714e-01, -1.7459e-01, -1.4012e-01,\n",
      "          2.7066e-01,  1.4523e-01, -1.5088e-01,  3.6761e-01, -3.8853e-02,\n",
      "          5.7292e-02, -2.7875e-01,  8.8071e-02, -1.4066e-01,  7.0193e-02,\n",
      "          3.9154e-02, -1.5142e-01,  5.2646e-01, -8.6247e-02, -4.1041e-01,\n",
      "          1.5252e-01,  1.0641e-01, -1.3845e-01, -3.2809e-02, -1.0649e-01,\n",
      "          4.6053e-05, -9.5015e-02,  1.7299e-01,  1.3369e-01,  4.9179e-02,\n",
      "         -2.1631e-01, -8.9424e-02, -1.7917e-01,  1.9961e-01, -1.1426e-01,\n",
      "          1.7963e-01, -3.0553e-02,  8.9062e-01, -5.8727e-01, -3.4824e-01,\n",
      "         -2.1484e-01, -7.2932e-02,  3.6915e-02,  7.3413e-02, -2.9696e-01,\n",
      "         -2.3906e-01, -2.1047e-01, -2.2001e-01, -4.5121e-01,  9.3490e-02,\n",
      "          4.8495e-01, -5.0648e-01,  1.0707e-01,  1.6763e-01, -3.8087e-02,\n",
      "          4.1458e-03, -1.5006e-01, -5.1166e-02, -3.8682e-01, -6.7020e-01,\n",
      "         -3.5458e-02, -2.5067e-01, -5.0097e-01, -2.3758e-01,  8.0010e-01,\n",
      "         -3.6116e-01, -1.1659e+00,  1.7047e-01, -3.3297e-01, -6.8924e-03,\n",
      "         -1.8426e-01,  8.1496e-02, -2.0214e-01, -3.6644e-01, -1.4743e-01,\n",
      "         -5.6160e-02, -5.3966e-01, -4.8913e-01, -8.6039e-01, -8.5426e-02,\n",
      "         -1.3030e-01, -2.8325e-01, -5.0701e-01,  1.1836e-01,  6.0275e-01,\n",
      "          2.0812e-01, -2.8729e-02,  1.1443e-01, -1.6701e-01,  2.1104e-01,\n",
      "          1.8469e-01,  1.0776e-01,  1.2261e-01,  3.5466e-01, -3.0019e-01,\n",
      "          5.4523e-01, -3.6841e-02, -4.0683e-01,  3.8250e-01, -4.8595e-01,\n",
      "          2.7327e-03,  1.0251e-01,  1.7638e-01, -1.6855e-01, -3.4550e-02,\n",
      "          1.6637e-01, -4.0273e-01, -1.7752e-01, -1.1324e-01,  4.6991e-03,\n",
      "          3.1124e-01,  4.5635e-02, -3.9149e-01, -7.7142e-02,  5.5214e-01,\n",
      "          1.6485e-01, -3.7989e-01,  3.7354e-01,  1.8476e-01,  3.0005e-01,\n",
      "         -1.9276e-01,  3.4097e-01, -3.1742e-01,  1.0602e-02, -2.5045e-01,\n",
      "         -8.0057e-02, -1.7604e-01,  1.0380e-01, -8.3757e-02, -1.1595e-01,\n",
      "         -2.6924e-01,  1.2829e-02,  2.8961e-01,  1.7831e-02,  4.9390e-01,\n",
      "          2.4894e-02,  9.9319e-02,  6.9069e-02,  5.8113e-01,  1.9371e-04,\n",
      "          4.1411e-01, -2.4053e-01,  2.3917e-01, -5.7677e-01,  7.6113e-02,\n",
      "          9.9587e-02, -2.4658e-01,  3.7114e-01, -4.3791e-01,  4.1817e-01,\n",
      "          2.6186e-01, -4.7087e-01, -6.0767e-01,  2.5010e-02, -1.6639e-03,\n",
      "          2.9469e-01, -1.2414e-01, -2.7272e-02,  2.0254e-01,  1.5458e-01,\n",
      "          3.1103e-01,  8.3651e-02, -3.1814e-01,  9.7555e-02,  8.6548e-02,\n",
      "          1.2220e-02, -4.7655e-01, -1.8052e-01,  5.3171e-01, -2.7044e-01,\n",
      "         -5.4548e-01, -1.8983e-01,  2.2678e-01, -1.1237e-01,  3.2577e-01,\n",
      "         -8.6579e-02, -5.0423e-02,  5.6979e-01,  1.2805e-01, -3.1528e-03,\n",
      "         -2.0454e-01, -1.6840e-01,  5.8266e-01, -5.2585e-01,  6.6153e-01,\n",
      "         -3.4998e-02, -4.3833e-01, -1.2301e-01, -2.8146e-01,  5.0005e-01,\n",
      "         -6.9900e-01, -7.3801e-02, -2.0160e-01, -5.7461e-02,  3.2661e-01,\n",
      "         -3.5047e-01,  3.6458e-01, -7.7929e-02, -2.2879e-01,  2.0457e-02,\n",
      "         -2.3809e-01, -4.2213e-02,  2.6812e-01, -4.2911e-02, -3.5272e-01,\n",
      "         -7.0648e-01,  1.4592e-03,  1.8226e-01,  3.4117e-02,  7.2401e-02,\n",
      "          1.9073e-01, -3.9710e-02,  4.2260e-02, -5.9521e-01,  3.1566e-01,\n",
      "          2.2617e-02,  5.4083e-01, -4.3649e-01, -6.5657e-02,  1.6362e-01,\n",
      "          1.0007e-01, -4.0593e-01, -3.1107e-01,  8.7609e-02, -4.8685e-01,\n",
      "          1.2170e-01,  1.6698e-01,  1.5294e-01,  2.2693e-01, -9.3500e-02,\n",
      "          3.6644e-02,  2.1878e-01, -4.9999e-02, -3.6267e-01, -1.5649e-01,\n",
      "         -3.1944e-01, -1.6561e-01,  6.9912e-01, -4.2960e-01, -3.9043e-01,\n",
      "          3.7306e-01, -6.0384e-01, -3.7884e-01, -1.7447e-01, -3.0349e-01,\n",
      "         -3.2082e-01, -2.7997e-01, -1.0869e-01, -1.2128e+00, -8.0879e-02,\n",
      "         -1.5023e-02, -2.9273e-01, -4.8775e-01, -9.2753e-02, -2.7411e-01,\n",
      "          2.8686e-01,  2.1735e-01, -1.3381e-02,  3.4396e-02,  2.1200e-01,\n",
      "         -1.7099e-01,  7.5704e-02, -3.5305e-01, -2.7081e-01, -4.7040e-02,\n",
      "          2.4592e-01,  1.3419e-01, -2.8241e-01, -2.7986e-02,  1.1832e-01,\n",
      "         -6.7709e-01, -1.3171e-01, -2.1398e-01,  9.3527e-02,  7.7502e-02,\n",
      "         -2.7786e-01, -2.0935e-01,  5.4520e-01, -4.9214e-01,  1.6408e-01,\n",
      "          4.8416e-01, -6.7268e-03, -1.4571e-01,  4.4787e-02,  1.1672e-01,\n",
      "         -4.7595e-02, -2.0143e-01,  2.6101e-01,  7.6596e-02,  1.1970e-01,\n",
      "         -1.0342e-01,  1.3092e-01,  1.3981e-01,  1.7972e-01,  1.2021e-01,\n",
      "          4.0582e-01,  5.0611e-02,  1.7104e-01, -1.0690e-01, -2.1472e-01,\n",
      "         -1.8136e-01, -6.2664e-02, -3.5671e-01, -5.9691e-01, -1.3561e-01,\n",
      "          5.7238e-01,  2.9241e-01, -7.4465e-01,  3.8130e-01, -1.0657e-01,\n",
      "         -7.2113e-01,  2.1953e-01,  3.8982e-01, -3.4576e-01,  1.3598e-01,\n",
      "          1.3947e-01, -1.2993e-01,  1.1005e-02,  5.0003e-01, -9.0967e-02,\n",
      "         -4.4596e-01,  2.6338e-01,  9.2785e-02, -3.2752e-01,  3.8488e-01,\n",
      "          1.2177e-01,  6.1829e-01,  8.2437e-02, -3.0834e-01, -1.0490e-01,\n",
      "          2.9929e-01,  7.5033e-02,  3.3668e-01,  1.3338e-01, -5.9178e-02,\n",
      "          1.9569e-01,  4.4784e-01,  1.6381e-01,  5.5058e-01,  2.2071e-01,\n",
      "          2.0972e-01,  2.6302e-01,  1.6316e-01,  1.7429e-01, -3.4065e-02,\n",
      "          1.5699e-01,  9.0890e-02,  1.8830e-01, -6.6941e-02,  3.8242e-01,\n",
      "          4.4645e-01,  6.8322e-02,  1.7052e-01,  3.4412e-01, -1.9854e-01,\n",
      "          1.0101e-01, -5.6051e-01, -2.2679e-02,  3.2171e-01,  1.1754e-01,\n",
      "         -3.8887e-01, -1.4632e-01,  3.0725e-02,  6.0685e-03,  4.4785e-01,\n",
      "         -2.2044e-01, -2.0336e-01, -5.7921e-02,  9.5157e-01,  2.1224e-01,\n",
      "         -3.5900e-02, -2.6911e-01, -2.2762e-01, -5.4031e-02,  2.2237e-02,\n",
      "         -9.6845e-02,  1.6002e-01,  4.9260e-02, -3.0762e-01,  6.8836e-02,\n",
      "          1.6883e-01,  1.6543e-02, -4.9646e-01,  1.0486e-01,  1.0962e-01,\n",
      "          8.5476e-01, -2.6182e-01, -2.0689e-01, -1.4931e-01,  2.3428e-01,\n",
      "          2.3717e-01,  3.4062e-01,  1.6488e-01, -1.1065e-01, -6.0524e-01,\n",
      "         -3.3644e-01, -3.4731e-01, -1.5921e-01,  2.8680e-01,  9.8491e-03,\n",
      "          1.3206e-01, -2.3973e-01,  1.6021e-01, -7.9825e-02,  7.6297e-02,\n",
      "         -6.2497e-01,  1.8182e-01, -9.6688e-02,  2.5841e-01,  3.6820e-01,\n",
      "         -1.2781e-01, -7.4040e-01,  7.8982e-02, -2.8848e-01,  9.2511e-02,\n",
      "         -1.1536e-01,  1.7778e-01, -2.2600e-02, -2.5044e-02, -1.7361e-01,\n",
      "          1.7621e-01,  2.0528e-01, -2.5509e-01, -3.4954e-01,  7.3535e-02,\n",
      "          2.9903e-02,  2.9630e-01, -2.0999e-01, -1.2038e-01,  7.1535e-02,\n",
      "          3.5688e-01,  2.5639e-01,  8.8033e-02,  2.6177e-01, -1.6622e-01,\n",
      "         -7.3090e-02, -7.3794e-02, -3.5848e-01, -1.7630e-03, -5.0627e-02,\n",
      "         -2.5208e-01,  1.0700e-01, -2.4137e-01, -1.2172e-01,  4.7188e-02,\n",
      "         -1.3551e-01, -6.4025e-01,  7.1211e-02,  1.1099e-01,  5.3232e-02,\n",
      "         -4.7867e-02,  2.3527e-01, -2.2400e-02]])\n"
     ]
    }
   ],
   "source": [
    "text = [\"GeeksforGeeks is a technology website\"]\n",
    "# Tokenize and encode text using batch_encode_plus\n",
    "# The function returns a dictionary containing the token IDs and attention masks\n",
    "encoding = tokenizer.batch_encode_plus(\n",
    "    text,                    # List of input texts\n",
    "    padding=True,              # Pad to the maximum sequence length\n",
    "    truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "    return_tensors='pt',      # Return PyTorch tensors\n",
    "    add_special_tokens=True    # Add special tokens CLS and SEP\n",
    ")\n",
    "            \n",
    "input_ids = encoding['input_ids']  # Token IDs\n",
    "            # print input IDs\n",
    "            #print(f\"Input ID: {input_ids}\")\n",
    "attention_mask = encoding['attention_mask']  # Attention mask\n",
    "            # print attention mask\n",
    "            #print(f\"Attention mask: {attention_mask}\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embeddings = outputs.last_hidden_state  # This contains the embeddings\n",
    "\n",
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "#print decoded text\n",
    "print(f\"Decoded Text: {decoded_text}\")\n",
    "# Tokenize the text again for reference\n",
    "tokenized_text = tokenizer.tokenize(decoded_text)\n",
    "#print tokenized text\n",
    "print(f\"tokenized Text: {tokenized_text}\")\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(text, return_tensors='pt')  # Returns a tensor\n",
    "sentence_embedding = word_embeddings.mean(dim=1)  # Average pooling along the sequence length dimension\n",
    " \n",
    "# Print the sentence embedding\n",
    "print(\"Sentence Embedding:\")\n",
    "print(sentence_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
